{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHQA with DSPy Optimization\n",
    "\n",
    "We'll use DSPy's teleprompters to automatically optimize prompts for better performance compared to the baseline GPT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 152 test examples\n",
      "Loaded 400 train examples\n",
      "Loaded 150 PQUAD examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import dspy\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "with open(\"../../../data/test_data.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(\"../../../data/train_data.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "pquad_df = pd.read_csv('../../../data/pquad/pquad_questions.csv', encoding='utf-8')\n",
    "pquad_data = pquad_df.to_dict(orient='records')[:150]  # Use first 150 samples\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples\")\n",
    "print(f\"Loaded {len(train_data)} train examples\") \n",
    "print(f\"Loaded {len(pquad_data)} PQUAD examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=f\"openai/{MODEL_NAME}\",\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"),\n",
    "    api_base=\"https://api.metisai.ir/openai/v1\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy signatures for Persian Question Answering\n",
    "class PersianQASignature(dspy.Signature):\n",
    "    \"\"\"Answer Persian questions concisely and accurately.\"\"\"\n",
    "    question = dspy.InputField(desc=\"Persian question to be answered\")\n",
    "    answer = dspy.OutputField(desc=\"Concise Persian answer\")\n",
    "\n",
    "class PersianQAWithReasoningSignature(dspy.Signature):\n",
    "    \"\"\"Answer Persian questions with step-by-step reasoning.\"\"\"\n",
    "    question = dspy.InputField(desc=\"Persian question to be answered\")\n",
    "    answer = dspy.OutputField(desc=\"Concise Persian answer after reasoning\")\n",
    "\n",
    "# DSPy modules\n",
    "class PersianQAModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(PersianQASignature)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        result = self.generate_answer(question=question)\n",
    "        return dspy.Prediction(answer=result.answer)\n",
    "\n",
    "class PersianQAWithReasoningModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(PersianQAWithReasoningSignature)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        result = self.generate_answer(question=question)\n",
    "        return dspy.Prediction(answer=result.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: تهران پایتخت کدام کشور است؟\n",
      "Answer: تهران پایتخت ایران است.\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "sample_question = \"تهران پایتخت کدام کشور است؟\"\n",
    "qa_module = PersianQAModule()\n",
    "result = qa_module(question=sample_question)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Answer: {result.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function using LLM-as-a-judge \n",
    "def create_llm_judge():\n",
    "    judge_lm = dspy.LM(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        api_key=os.getenv(\"METIS_API_KEY\"), \n",
    "        api_base=\"https://api.metisai.ir/openai/v1\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return judge_lm\n",
    "\n",
    "def clean_model_answer(model_answer: str) -> str:\n",
    "    \"\"\"Clean model answer by removing XML tags\"\"\"\n",
    "    cleaned = re.sub(r'<ANSWER>(.*?)</ANSWER>', r'\\1', model_answer, flags=re.DOTALL|re.IGNORECASE)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def evaluate_answer_with_judge(question: str, correct_answer: str, model_answer: str, judge_lm) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert judge evaluating Persian/Farsi question-answer pairs. Determine if the model's answer is semantically equivalent to the correct answer. Be strict but fair about minor spelling variations and equivalent expressions.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Correct Answer: {correct_answer}\n",
    "        Model Answer: {clean_model_answer(model_answer)}\n",
    "\n",
    "        Answer only \"TRUE\" if the model answer is semantically equivalent to the correct answer, or \"FALSE\" otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = judge_lm(prompt)\n",
    "        if isinstance(response, list) and len(response) > 0:\n",
    "            response_text = str(response[0])\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "        \n",
    "        # print(f\"Judge response: {response_text}\")  # Debug output\n",
    "        return \"TRUE\" in response_text.upper()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in judge evaluation: {e}\")  # Debug output\n",
    "        return False\n",
    "\n",
    "# DSPy evaluation metric\n",
    "def accuracy_metric(gold, pred, trace=None):\n",
    "    judge_lm = create_llm_judge()\n",
    "    return evaluate_answer_with_judge(gold.question, gold.answer, pred.answer, judge_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: تهران پایتخت کدام کشور است؟\n",
      "Expected Answer: ایران\n",
      "Model Answer: تهران پایتخت ایران است.\n",
      "Cleaned Model Answer: تهران پایتخت ایران است.\n",
      "\n",
      "==================================================\n",
      "Testing LLM Judge:\n",
      "==================================================\n",
      "Judge response: TRUE\n",
      "Final evaluation result: True\n"
     ]
    }
   ],
   "source": [
    "## test the evaluation function\n",
    "sample_question = \"تهران پایتخت کدام کشور است؟\"\n",
    "sample_answer = \"ایران\"\n",
    "\n",
    "# Test the QA module\n",
    "qa_module = PersianQAModule()\n",
    "result = qa_module(question=sample_question)\n",
    "\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Expected Answer: {sample_answer}\")\n",
    "print(f\"Model Answer: {result.answer}\")\n",
    "print(f\"Cleaned Model Answer: {clean_model_answer(result.answer)}\")\n",
    "\n",
    "# Test the judge\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing LLM Judge:\")\n",
    "print(\"=\"*50)\n",
    "try:\n",
    "    judge_lm = create_llm_judge()\n",
    "    is_correct = evaluate_answer_with_judge(sample_question, sample_answer, result.answer, judge_lm)\n",
    "    print(f\"Final evaluation result: {is_correct}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge response: FALSE\n",
      "Final evaluation result: False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    judge_lm = create_llm_judge()\n",
    "    is_correct = evaluate_answer_with_judge(sample_question, sample_answer, \"فرانسه\", judge_lm)\n",
    "    print(f\"Final evaluation result: {is_correct}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHQA Dataset Experiments\n",
    "\n",
    "We'll first work with the Persian Multi-Hop Question Answering dataset, optimizing prompts for both reasoning and non-reasoning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHQA Train examples: 50\n",
      "MHQA Test examples: 152\n",
      "\n",
      "Sample train example:\n",
      "Question: در مقابل تبی که بنتونیت با نام گل ارمنی در آن معروف است چه چیز قرار دارد ؟\n",
      "Answer:  پزشکی مبتنی بر شواهد\n"
     ]
    }
   ],
   "source": [
    "# Prepare MHQA data for DSPy\n",
    "def prepare_dspy_examples(data_list):\n",
    "    examples = []\n",
    "    for item in data_list:\n",
    "        example = dspy.Example(\n",
    "            question=item['question'],\n",
    "            answer=item['answer']\n",
    "        ).with_inputs('question')\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "mhqa_train_examples = prepare_dspy_examples(train_data[:50])  # Use first 50 for training\n",
    "mhqa_test_examples = prepare_dspy_examples(test_data)\n",
    "\n",
    "print(f\"MHQA Train examples: {len(mhqa_train_examples)}\")\n",
    "print(f\"MHQA Test examples: {len(mhqa_test_examples)}\")\n",
    "\n",
    "print(f\"\\nSample train example:\")\n",
    "print(f\"Question: {mhqa_train_examples[0].question}\")\n",
    "print(f\"Answer: {mhqa_train_examples[0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHQA - No Reasoning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MHQA No Reasoning optimization...\n",
      "Optimizing prompts for MHQA no reasoning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:25<01:56,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 9 examples for up to 2 rounds, amounting to 14 attempts.\n",
      "MHQA No Reasoning optimization completed!\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED PROMPT FOR MHQA NO REASONING:\n",
      "==================================================\n",
      "\n",
      "Predictor 1:\n",
      "PersianQASignature(question -> answer\n",
      "    instructions='Answer Persian questions concisely and accurately.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Number of demonstrations: 4\n",
      "Demo 1: Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟...\n",
      "         A: بله، سایتو دوسان یک نجیب زاده نظامی است.\n",
      "Demo 2: Q: کدام یک از مواردی که سشات در آن به عنوان ایزدبانو شناخته می شد فن محاسبه اعداد می باشد ؟...\n",
      "         A: ریاضیات\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting MHQA No Reasoning optimization...\")\n",
    "\n",
    "mhqa_no_reasoning_model = PersianQAModule()\n",
    "\n",
    "# Configure teleprompter (optimizer)\n",
    "teleprompter = dspy.BootstrapFewShot(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=2\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "print(\"Optimizing prompts for MHQA no reasoning...\")\n",
    "mhqa_optimized_no_reasoning = teleprompter.compile(\n",
    "    mhqa_no_reasoning_model, \n",
    "    trainset=mhqa_train_examples\n",
    ")\n",
    "\n",
    "print(\"MHQA No Reasoning optimization completed!\")\n",
    "\n",
    "# Show optimized prompt\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED PROMPT FOR MHQA NO REASONING:\")\n",
    "print(\"=\"*50)\n",
    "for i, predictor in enumerate(mhqa_optimized_no_reasoning.predictors()):\n",
    "    print(f\"\\nPredictor {i+1}:\")\n",
    "    print(predictor.signature)\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(f\"Number of demonstrations: {len(predictor.demos)}\")\n",
    "        for j, demo in enumerate(predictor.demos[:2]):  # Show first 2 demos\n",
    "            print(f\"Demo {j+1}: Q: {demo.question[:100]}...\")\n",
    "            print(f\"         A: {demo.answer}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MHQA No Reasoning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MHQA No Reasoning: 100%|██████████| 152/152 [06:47<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MHQA No Reasoning Accuracy: 0.441\n",
      "Correct answers: 67/152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MHQA no reasoning model\n",
    "print(\"Evaluating MHQA No Reasoning model...\")\n",
    "\n",
    "mhqa_no_reasoning_results = []\n",
    "judge_lm = create_llm_judge()\n",
    "\n",
    "for example in tqdm(mhqa_test_examples, desc=\"Testing MHQA No Reasoning\"):\n",
    "    try:\n",
    "        prediction = mhqa_optimized_no_reasoning(question=example.question)\n",
    "        model_answer = prediction.answer\n",
    "    except Exception as e:\n",
    "        model_answer = f\"Error: {e}\"\n",
    "    \n",
    "    # Evaluate with LLM judge\n",
    "    is_correct = evaluate_answer_with_judge(\n",
    "        example.question, \n",
    "        example.answer, \n",
    "        model_answer, \n",
    "        judge_lm\n",
    "    )\n",
    "    \n",
    "    mhqa_no_reasoning_results.append({\n",
    "        'question': example.question,\n",
    "        'answer': example.answer,\n",
    "        'model_answer': model_answer,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': is_correct,\n",
    "        'id': getattr(example, 'id', None)\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "mhqa_no_reasoning_df = pd.DataFrame(mhqa_no_reasoning_results)\n",
    "mhqa_no_reasoning_df.to_csv('mhqa_dspy_no_reasoning_results.csv', index=False)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = len(mhqa_no_reasoning_df[mhqa_no_reasoning_df['is_correct'] == True]) / len(mhqa_no_reasoning_df)\n",
    "print(f\"\\nMHQA No Reasoning Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Correct answers: {len(mhqa_no_reasoning_df[mhqa_no_reasoning_df['is_correct'] == True])}/{len(mhqa_no_reasoning_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHQA - With Reasoning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MHQA With Reasoning optimization...\n",
      "Optimizing prompts for MHQA with reasoning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:47<04:11,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples for up to 2 rounds, amounting to 13 attempts.\n",
      "MHQA With Reasoning optimization completed!\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED PROMPT FOR MHQA WITH REASONING:\n",
      "==================================================\n",
      "\n",
      "Predictor 1:\n",
      "StringSignature(question -> reasoning, answer\n",
      "    instructions='Answer Persian questions with step-by-step reasoning.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer after reasoning', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Number of demonstrations: 4\n",
      "Demo 1: Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟...\n",
      "         A: بله، سایتو دوسان یک نجیب‌زاده نظامی است.\n",
      "Demo 2: Q: کدام یک از بازیگران فیلم اثر پروانه‌ای حرف مدلینگ را در کشور ایتالیا شروع کرد ؟...\n",
      "         A: امی اسمارت\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting MHQA With Reasoning optimization...\")\n",
    "\n",
    "# Create baseline model with reasoning\n",
    "mhqa_reasoning_model = PersianQAWithReasoningModule()\n",
    "\n",
    "# Configure teleprompter for reasoning approach\n",
    "teleprompter_reasoning = dspy.BootstrapFewShot(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=2\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "print(\"Optimizing prompts for MHQA with reasoning...\")\n",
    "mhqa_optimized_reasoning = teleprompter_reasoning.compile(\n",
    "    mhqa_reasoning_model, \n",
    "    trainset=mhqa_train_examples\n",
    ")\n",
    "\n",
    "print(\"MHQA With Reasoning optimization completed!\")\n",
    "\n",
    "# Show optimized prompt\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED PROMPT FOR MHQA WITH REASONING:\")\n",
    "print(\"=\"*50)\n",
    "for i, predictor in enumerate(mhqa_optimized_reasoning.predictors()):\n",
    "    print(f\"\\nPredictor {i+1}:\")\n",
    "    print(predictor.signature)\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(f\"Number of demonstrations: {len(predictor.demos)}\")\n",
    "        for j, demo in enumerate(predictor.demos[:2]):  # Show first 2 demos\n",
    "            print(f\"Demo {j+1}: Q: {demo.question[:100]}...\")\n",
    "            print(f\"         A: {demo.answer}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MHQA With Reasoning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MHQA With Reasoning:  97%|█████████▋| 148/152 [09:06<00:14,  3.69s/it]2025/09/06 11:17:46 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing MHQA With Reasoning: 100%|██████████| 152/152 [09:20<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MHQA With Reasoning Accuracy: 0.454\n",
      "Correct answers: 69/152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating MHQA With Reasoning model...\")\n",
    "\n",
    "mhqa_reasoning_results = []\n",
    "judge_lm = create_llm_judge()\n",
    "\n",
    "for example in tqdm(mhqa_test_examples, desc=\"Testing MHQA With Reasoning\"):\n",
    "    try:\n",
    "        prediction = mhqa_optimized_reasoning(question=example.question)\n",
    "        model_answer = prediction.answer\n",
    "    except Exception as e:\n",
    "        model_answer = f\"Error: {e}\"\n",
    "    \n",
    "    # Evaluate with LLM judge\n",
    "    is_correct = evaluate_answer_with_judge(\n",
    "        example.question, \n",
    "        example.answer, \n",
    "        model_answer, \n",
    "        judge_lm\n",
    "    )\n",
    "    \n",
    "    mhqa_reasoning_results.append({\n",
    "        'question': example.question,\n",
    "        'answer': example.answer,\n",
    "        'model_answer': model_answer,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': is_correct,\n",
    "        'id': getattr(example, 'id', None)\n",
    "    })\n",
    "\n",
    "mhqa_reasoning_df = pd.DataFrame(mhqa_reasoning_results)\n",
    "mhqa_reasoning_df.to_csv('mhqa_dspy_reasoning_results.csv', index=False)\n",
    "\n",
    "accuracy_reasoning = len(mhqa_reasoning_df[mhqa_reasoning_df['is_correct'] == True]) / len(mhqa_reasoning_df)\n",
    "print(f\"\\nMHQA With Reasoning Accuracy: {accuracy_reasoning:.3f}\")\n",
    "print(f\"Correct answers: {len(mhqa_reasoning_df[mhqa_reasoning_df['is_correct'] == True])}/{len(mhqa_reasoning_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQUAD Dataset Experiments\n",
    "\n",
    "Now we'll work with the PQUAD dataset, following the same optimization approach for both reasoning and non-reasoning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQUAD Train examples: 30\n",
      "PQUAD Test examples: 120\n",
      "\n",
      "Sample PQUAD example:\n",
      "Question: ساختار آب چیست؟\n",
      "Answer: اکسید هیدروژن\n"
     ]
    }
   ],
   "source": [
    "pquad_examples = prepare_dspy_examples(pquad_data)\n",
    "\n",
    "# Split PQUAD data for training and testing\n",
    "pquad_train_examples = pquad_examples[:30]  # Use first 30 for training\n",
    "pquad_test_examples = pquad_examples[30:]   # Rest for testing (150-30 =120)\n",
    "\n",
    "print(f\"PQUAD Train examples: {len(pquad_train_examples)}\")\n",
    "print(f\"PQUAD Test examples: {len(pquad_test_examples)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample PQUAD example:\")\n",
    "print(f\"Question: {pquad_train_examples[0].question}\")\n",
    "print(f\"Answer: {pquad_train_examples[0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQUAD - No Reasoning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PQUAD No Reasoning optimization...\n",
      "Optimizing prompts for PQUAD no reasoning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:18<01:18,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 15 examples for up to 2 rounds, amounting to 27 attempts.\n",
      "PQUAD No Reasoning optimization completed!\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED PROMPT FOR PQUAD NO REASONING:\n",
      "==================================================\n",
      "\n",
      "Predictor 1:\n",
      "PersianQASignature(question -> answer\n",
      "    instructions='Answer Persian questions concisely and accurately.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Number of demonstrations: 4\n",
      "Demo 1: Q: ساختار آب چیست؟...\n",
      "         A: آب از دو اتم هیدروژن و یک اتم اکسیژن تشکیل شده است (H₂O).\n",
      "Demo 2: Q: موقعیت جغرافیایی کلیسای کاتولیک رم چگونه است؟...\n",
      "         A: کلیسای کاتولیک رم در شهر واتیکان واقع در ایتالیا قرار دارد و به عنوان مرکز کلیسای کاتولیک نیز شناخته می‌شود.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting PQUAD No Reasoning optimization...\")\n",
    "\n",
    "pquad_no_reasoning_model = PersianQAModule()\n",
    "\n",
    "# Configure teleprompter for PQUAD\n",
    "teleprompter_pquad = dspy.BootstrapFewShot(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=2\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "print(\"Optimizing prompts for PQUAD no reasoning...\")\n",
    "pquad_optimized_no_reasoning = teleprompter_pquad.compile(\n",
    "    pquad_no_reasoning_model, \n",
    "    trainset=pquad_train_examples\n",
    ")\n",
    "\n",
    "print(\"PQUAD No Reasoning optimization completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED PROMPT FOR PQUAD NO REASONING:\")\n",
    "print(\"=\"*50)\n",
    "for i, predictor in enumerate(pquad_optimized_no_reasoning.predictors()):\n",
    "    print(f\"\\nPredictor {i+1}:\")\n",
    "    print(predictor.signature)\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(f\"Number of demonstrations: {len(predictor.demos)}\")\n",
    "        for j, demo in enumerate(predictor.demos[:2]):  # Show first 2 demos\n",
    "            print(f\"Demo {j+1}: Q: {demo.question[:100]}...\")\n",
    "            print(f\"         A: {demo.answer}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PQUAD No Reasoning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing PQUAD No Reasoning: 100%|██████████| 120/120 [06:32<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PQUAD No Reasoning Accuracy: 0.233\n",
      "Correct answers: 28/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating PQUAD No Reasoning model...\")\n",
    "\n",
    "pquad_no_reasoning_results = []\n",
    "judge_lm = create_llm_judge()\n",
    "\n",
    "for example in tqdm(pquad_test_examples, desc=\"Testing PQUAD No Reasoning\"):\n",
    "    try:\n",
    "        prediction = pquad_optimized_no_reasoning(question=example.question)\n",
    "        model_answer = prediction.answer\n",
    "    except Exception as e:\n",
    "        model_answer = f\"Error: {e}\"\n",
    "    \n",
    "    # Evaluate with LLM judge\n",
    "    is_correct = evaluate_answer_with_judge(\n",
    "        example.question, \n",
    "        example.answer, \n",
    "        model_answer, \n",
    "        judge_lm\n",
    "    )\n",
    "    \n",
    "    pquad_no_reasoning_results.append({\n",
    "        'question': example.question,\n",
    "        'answer': example.answer,\n",
    "        'model_answer': model_answer,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': is_correct,\n",
    "        'id': getattr(example, 'id', None)\n",
    "    })\n",
    "\n",
    "pquad_no_reasoning_df = pd.DataFrame(pquad_no_reasoning_results)\n",
    "pquad_no_reasoning_df.to_csv('pquad_dspy_no_reasoning_results.csv', index=False)\n",
    "\n",
    "# Calculate accuracy\n",
    "pquad_accuracy = len(pquad_no_reasoning_df[pquad_no_reasoning_df['is_correct'] == True]) / len(pquad_no_reasoning_df)\n",
    "print(f\"\\nPQUAD No Reasoning Accuracy: {pquad_accuracy:.3f}\")\n",
    "print(f\"Correct answers: {len(pquad_no_reasoning_df[pquad_no_reasoning_df['is_correct'] == True])}/{len(pquad_no_reasoning_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQUAD - With Reasoning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PQUAD With Reasoning optimization...\n",
      "Optimizing prompts for PQUAD with reasoning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [02:17<02:00,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 16 examples for up to 2 rounds, amounting to 29 attempts.\n",
      "PQUAD With Reasoning optimization completed!\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED PROMPT FOR PQUAD WITH REASONING:\n",
      "==================================================\n",
      "\n",
      "Predictor 1:\n",
      "StringSignature(question -> reasoning, answer\n",
      "    instructions='Answer Persian questions with step-by-step reasoning.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer after reasoning', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Number of demonstrations: 4\n",
      "Demo 1: Q: مهمترین آثار باستانی استان گیلان کدام است؟...\n",
      "         A: قلعه رودخان، بقعه شیخ زاهد گیلانی، و تپه‌های باستانی چمخاله.\n",
      "Demo 2: Q: موقعیت جغرافیایی کلیسای کاتولیک رم چگونه است؟...\n",
      "         A: کلیسای کاتولیک رم در شهر واتیکان در درون رم، ایتالیا واقع شده است.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting PQUAD With Reasoning optimization...\")\n",
    "\n",
    "pquad_reasoning_model = PersianQAWithReasoningModule()\n",
    "\n",
    "# Configure teleprompter for reasoning approach\n",
    "teleprompter_pquad_reasoning = dspy.BootstrapFewShot(\n",
    "    metric=accuracy_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=2\n",
    ")\n",
    "\n",
    "# Optimize the model\n",
    "print(\"Optimizing prompts for PQUAD with reasoning...\")\n",
    "pquad_optimized_reasoning = teleprompter_pquad_reasoning.compile(\n",
    "    pquad_reasoning_model, \n",
    "    trainset=pquad_train_examples\n",
    ")\n",
    "\n",
    "print(\"PQUAD With Reasoning optimization completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED PROMPT FOR PQUAD WITH REASONING:\")\n",
    "print(\"=\"*50)\n",
    "for i, predictor in enumerate(pquad_optimized_reasoning.predictors()):\n",
    "    print(f\"\\nPredictor {i+1}:\")\n",
    "    print(predictor.signature)\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(f\"Number of demonstrations: {len(predictor.demos)}\")\n",
    "        for j, demo in enumerate(predictor.demos[:2]):  # Show first 2 demos\n",
    "            print(f\"Demo {j+1}: Q: {demo.question[:100]}...\")\n",
    "            print(f\"         A: {demo.answer}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PQUAD With Reasoning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing PQUAD With Reasoning:   5%|▌         | 6/120 [00:27<08:50,  4.66s/it]2025/09/06 11:33:40 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning:  26%|██▌       | 31/120 [02:26<06:40,  4.50s/it]2025/09/06 11:35:38 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning:  34%|███▍      | 41/120 [03:17<06:50,  5.20s/it]2025/09/06 11:36:30 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning:  55%|█████▌    | 66/120 [05:31<04:27,  4.96s/it]2025/09/06 11:38:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning:  76%|███████▌  | 91/120 [07:42<02:21,  4.87s/it]2025/09/06 11:40:55 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning:  81%|████████  | 97/120 [08:15<01:57,  5.10s/it]2025/09/06 11:41:31 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=200. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "Testing PQUAD With Reasoning: 100%|██████████| 120/120 [10:28<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PQUAD With Reasoning Accuracy: 0.233\n",
      "Correct answers: 28/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating PQUAD With Reasoning model...\")\n",
    "\n",
    "pquad_reasoning_results = []\n",
    "judge_lm = create_llm_judge()\n",
    "\n",
    "for example in tqdm(pquad_test_examples, desc=\"Testing PQUAD With Reasoning\"):\n",
    "    try:\n",
    "        prediction = pquad_optimized_reasoning(question=example.question)\n",
    "        model_answer = prediction.answer\n",
    "    except Exception as e:\n",
    "        model_answer = f\"Error: {e}\"\n",
    "    \n",
    "    # Evaluate with LLM judge\n",
    "    is_correct = evaluate_answer_with_judge(\n",
    "        example.question, \n",
    "        example.answer, \n",
    "        model_answer, \n",
    "        judge_lm\n",
    "    )\n",
    "    \n",
    "    pquad_reasoning_results.append({\n",
    "        'question': example.question,\n",
    "        'answer': example.answer,\n",
    "        'model_answer': model_answer,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': is_correct,\n",
    "        'id': getattr(example, 'id', None)\n",
    "    })\n",
    "\n",
    "pquad_reasoning_df = pd.DataFrame(pquad_reasoning_results)\n",
    "pquad_reasoning_df.to_csv('pquad_dspy_reasoning_results.csv', index=False)\n",
    "\n",
    "# Calculate accuracy\n",
    "pquad_accuracy_reasoning = len(pquad_reasoning_df[pquad_reasoning_df['is_correct'] == True]) / len(pquad_reasoning_df)\n",
    "print(f\"\\nPQUAD With Reasoning Accuracy: {pquad_accuracy_reasoning:.3f}\")\n",
    "print(f\"Correct answers: {len(pquad_reasoning_df[pquad_reasoning_df['is_correct'] == True])}/{len(pquad_reasoning_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary\n",
    "\n",
    "Let's compare all the results and examine the optimized prompts that DSPy discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL RESULTS SUMMARY - DSPy Optimized Models\n",
      "======================================================================\n",
      "\n",
      "MHQA Dataset Results:\n",
      "  • No Reasoning:  0.441 \n",
      "  • With Reasoning: 0.454\n",
      "\n",
      "PQUAD Dataset Results:\n",
      "  • No Reasoning:  0.233 \n",
      "  • With Reasoning: 0.233 \n",
      "\n",
      "======================================================================\n",
      "COMPARISON WITH BASELINE GPT RESULTS\n",
      "======================================================================\n",
      "\n",
      "MHQA Dataset Results:\n",
      "  • No Reasoning:  0.453 \n",
      "  • With Reasoning: 0.447 \n",
      "\n",
      "PQUAD Dataset Results:\n",
      "  • No Reasoning:  0.187 \n",
      "  • With Reasoning: 0.194\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY - DSPy Optimized Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nMHQA Dataset Results:\")\n",
    "print(f\"  • No Reasoning:  {accuracy:.3f} \")\n",
    "print(f\"  • With Reasoning: {accuracy_reasoning:.3f}\")\n",
    "\n",
    "print(f\"\\nPQUAD Dataset Results:\")\n",
    "print(f\"  • No Reasoning:  {pquad_accuracy:.3f} \")\n",
    "print(f\"  • With Reasoning: {pquad_accuracy_reasoning:.3f} \")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON WITH BASELINE GPT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nMHQA Dataset Results:\")\n",
    "print(f\"  • No Reasoning:  {0.453} \")\n",
    "print(f\"  • With Reasoning: {0.447} \")\n",
    "\n",
    "print(f\"\\nPQUAD Dataset Results:\")\n",
    "print(f\"  • No Reasoning:  {0.187} \")\n",
    "print(f\"  • With Reasoning: {0.194}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DETAILED PROMPT INSPECTION: MHQA No Reasoning\n",
      "==================================================\n",
      "\n",
      "--- Predictor 1 ---\n",
      "Signature: PersianQASignature(question -> answer\n",
      "    instructions='Answer Persian questions concisely and accurately.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Demonstrations found: 4\n",
      "\n",
      "Few-shot examples selected by DSPy:\n",
      "\n",
      "Example 1:\n",
      "Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟\n",
      "A: بله، سایتو دوسان یک نجیب زاده نظامی است.\n",
      "\n",
      "Example 2:\n",
      "Q: کدام یک از مواردی که سشات در آن به عنوان ایزدبانو شناخته می شد فن محاسبه اعداد می باشد ؟\n",
      "A: ریاضیات\n",
      "\n",
      "Example 3:\n",
      "Q: کارگردان فیلم شب یلدا دانش آموخته چه رشته ای می باشد ؟\n",
      "A: رشته کارگردانی سینما\n",
      "\n",
      "Example 4:\n",
      "Q: آیا جایزه ای که سیدیبه در سال 2007 آن را دریافت کرد اکنون به عنوان یکی از معتبرترین و برجسته‌ترین جوایز صنعت سینما شناخته می‌شود ؟\n",
      "A: بله، جایزه‌ای که سیدیبه در سال 2007 دریافت کرد، جایزه اسکار است و به عنوان یکی از معتبرترین جوایز سینما شناخته می‌شود.\n",
      "... and 0 more examples\n",
      "\n",
      "==================================================\n",
      "DETAILED PROMPT INSPECTION: MHQA With Reasoning\n",
      "==================================================\n",
      "\n",
      "--- Predictor 1 ---\n",
      "Signature: StringSignature(question -> reasoning, answer\n",
      "    instructions='Answer Persian questions with step-by-step reasoning.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer after reasoning', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Demonstrations found: 4\n",
      "\n",
      "Few-shot examples selected by DSPy:\n",
      "\n",
      "Example 1:\n",
      "Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟\n",
      "A: بله، سایتو دوسان یک نجیب‌زاده نظامی است.\n",
      "\n",
      "Example 2:\n",
      "Q: کدام یک از بازیگران فیلم اثر پروانه‌ای حرف مدلینگ را در کشور ایتالیا شروع کرد ؟\n",
      "A: امی اسمارت\n",
      "\n",
      "Example 3:\n",
      "Q: کدام یک از مواردی که سشات در آن به عنوان ایزدبانو شناخته می شد فن محاسبه اعداد می باشد ؟\n",
      "A: فن محاسبه اعداد\n",
      "\n",
      "Example 4:\n",
      "Q: کارگردان فیلم شب یلدا دانش آموخته چه رشته ای می باشد ؟\n",
      "A: رشته کارگردانی و فیلمسازی\n",
      "... and 0 more examples\n",
      "\n",
      "==================================================\n",
      "DETAILED PROMPT INSPECTION: PQUAD No Reasoning\n",
      "==================================================\n",
      "\n",
      "--- Predictor 1 ---\n",
      "Signature: PersianQASignature(question -> answer\n",
      "    instructions='Answer Persian questions concisely and accurately.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Demonstrations found: 4\n",
      "\n",
      "Few-shot examples selected by DSPy:\n",
      "\n",
      "Example 1:\n",
      "Q: ساختار آب چیست؟\n",
      "A: آب از دو اتم هیدروژن و یک اتم اکسیژن تشکیل شده است (H₂O).\n",
      "\n",
      "Example 2:\n",
      "Q: موقعیت جغرافیایی کلیسای کاتولیک رم چگونه است؟\n",
      "A: کلیسای کاتولیک رم در شهر واتیکان واقع در ایتالیا قرار دارد و به عنوان مرکز کلیسای کاتولیک نیز شناخته می‌شود.\n",
      "\n",
      "Example 3:\n",
      "Q: رم در کدام قاره قرار گرفته‌است؟\n",
      "A: رم در قاره اروپا قرار گرفته‌است.\n",
      "\n",
      "Example 4:\n",
      "Q: سالانه حدود چند زمین لرزه در ژاپن رخ می‌دهد؟\n",
      "A: حدود ۱۰۰۰ تا ۱۵۰۰ زمین لرزه\n",
      "... and 0 more examples\n",
      "\n",
      "==================================================\n",
      "DETAILED PROMPT INSPECTION: PQUAD With Reasoning\n",
      "==================================================\n",
      "\n",
      "--- Predictor 1 ---\n",
      "Signature: StringSignature(question -> reasoning, answer\n",
      "    instructions='Answer Persian questions with step-by-step reasoning.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'desc': 'Persian question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Concise Persian answer after reasoning', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")\n",
      "Demonstrations found: 4\n",
      "\n",
      "Few-shot examples selected by DSPy:\n",
      "\n",
      "Example 1:\n",
      "Q: مهمترین آثار باستانی استان گیلان کدام است؟\n",
      "A: قلعه رودخان، بقعه شیخ زاهد گیلانی، و تپه‌های باستانی چمخاله.\n",
      "\n",
      "Example 2:\n",
      "Q: موقعیت جغرافیایی کلیسای کاتولیک رم چگونه است؟\n",
      "A: کلیسای کاتولیک رم در شهر واتیکان در درون رم، ایتالیا واقع شده است.\n",
      "\n",
      "Example 3:\n",
      "Q: رم در کدام قاره قرار گرفته‌است؟\n",
      "A: رم در قاره اروپا قرار دارد.\n",
      "\n",
      "Example 4:\n",
      "Q: گیلکی از نظر زبان شناسی به کجا تعلق دارد؟\n",
      "A: گیلکی به گروه زبان‌های ایرانی شمال غربی تعلق دارد.\n",
      "... and 0 more examples\n"
     ]
    }
   ],
   "source": [
    "def inspect_optimized_model(model, title):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DETAILED PROMPT INSPECTION: {title}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for i, predictor in enumerate(model.predictors()):\n",
    "        print(f\"\\n--- Predictor {i+1} ---\")\n",
    "        print(f\"Signature: {predictor.signature}\")\n",
    "        \n",
    "        if hasattr(predictor, 'demos') and predictor.demos:\n",
    "            print(f\"Demonstrations found: {len(predictor.demos)}\")\n",
    "            print(\"\\nFew-shot examples selected by DSPy:\")\n",
    "            for j, demo in enumerate(predictor.demos):\n",
    "                print(f\"\\nExample {j+1}:\")\n",
    "                print(f\"Q: {demo.question}\")\n",
    "                print(f\"A: {demo.answer}\")\n",
    "                if j >= 3:  # Show max 4 examples\n",
    "                    print(f\"... and {len(predictor.demos) - 4} more examples\")\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No demonstrations found\")\n",
    "        \n",
    "        if hasattr(predictor, 'extended_signature'):\n",
    "            print(f\"\\nExtended signature: {predictor.extended_signature}\")\n",
    "\n",
    "# Inspect all optimized models\n",
    "inspect_optimized_model(mhqa_optimized_no_reasoning, \"MHQA No Reasoning\")\n",
    "inspect_optimized_model(mhqa_optimized_reasoning, \"MHQA With Reasoning\") \n",
    "inspect_optimized_model(pquad_optimized_no_reasoning, \"PQUAD No Reasoning\")\n",
    "inspect_optimized_model(pquad_optimized_reasoning, \"PQUAD With Reasoning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
