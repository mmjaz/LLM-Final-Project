{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e47812",
   "metadata": {},
   "source": [
    "# Calculate EM, Precision, Recall, and F1 Score for PQuad Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830667ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e1e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/Baseline/google_api/pqaud_evaluated_results_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdac3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation + \"،\")\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def convert_digits_en2fa(text):\n",
    "    english_digits = '0123456789'\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹'\n",
    "    \n",
    "    translation_table = str.maketrans(english_digits, persian_digits)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "\n",
    "def parse_result(gold, generated_text) -> bool:\n",
    "    if pd.isna(gold) or pd.isna(generated_text):\n",
    "        return False\n",
    "        \n",
    "    gold = str(gold)\n",
    "    generated_text = str(generated_text)\n",
    "    \n",
    "    gold = convert_digits_en2fa(gold)\n",
    "    generated_text = convert_digits_en2fa(generated_text)\n",
    "\n",
    "    if gold in generated_text:\n",
    "        return True\n",
    "    if gold == \"بلی\":\n",
    "        if \"بله\" in generated_text:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4bd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_answer(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove <ANSWER> and </ANSWER> tags\n",
    "    text = re.sub(r'<ANSWER>', '', text)\n",
    "    text = re.sub(r'</ANSWER>', '', text)\n",
    "    # Clean up any extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20643396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    # Handle None/NaN values\n",
    "    if pd.isna(prediction) or pd.isna(ground_truth):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    prediction = str(prediction)\n",
    "    ground_truth = str(ground_truth)\n",
    "    \n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    # Handle special cases for yes/no answers\n",
    "    if normalized_prediction in ['بله', 'خیر', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['بله', 'خیر', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    \n",
    "    # If either is empty, return zeros\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return ZERO_METRIC\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    # Handle None/NaN values\n",
    "    if pd.isna(prediction) or pd.isna(ground_truth):\n",
    "        return False\n",
    "    \n",
    "    prediction = str(prediction)\n",
    "    ground_truth = str(ground_truth)\n",
    "    \n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715a73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each row\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "exact_matches = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    prediction = clean_model_answer(row['model_answer'])\n",
    "    ground_truth = row['answer']\n",
    "    \n",
    "    # Calculate F1, precision, recall\n",
    "    f1, precision, recall = f1_score(prediction, ground_truth)\n",
    "    f1_scores.append(f1)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n",
    "    # Calculate exact match\n",
    "    em = exact_match_score(prediction, ground_truth)\n",
    "    exact_matches.append(em)\n",
    "\n",
    "# Add to dataframe\n",
    "df['f1_score'] = f1_scores\n",
    "df['precision'] = precisions\n",
    "df['recall'] = recalls\n",
    "df['exact_match'] = exact_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba721ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MHQA Baseline Evaluation Results\n",
      "============================================================\n",
      "Total samples: 500\n",
      "----------------------------------------\n",
      "Exact Match (EM): 0.0760 (7.60%)\n",
      "F1 Score:         0.2034\n",
      "Precision:        0.2405\n",
      "Recall:           0.2099\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics\n",
    "overall_f1 = np.mean(f1_scores)\n",
    "overall_precision = np.mean(precisions)\n",
    "overall_recall = np.mean(recalls)\n",
    "overall_em = np.mean(exact_matches)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MHQA Baseline Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Exact Match (EM): {overall_em:.4f} ({overall_em*100:.2f}%)\")\n",
    "print(f\"F1 Score:         {overall_f1:.4f}\")\n",
    "print(f\"Precision:        {overall_precision:.4f}\")\n",
    "print(f\"Recall:           {overall_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aa467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
