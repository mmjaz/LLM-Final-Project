{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dotenv\n",
    "\n",
    "with open(\"../../data/test_data.json\", \"r\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "test_data = json.loads(test_data)\n",
    "dotenv.load_dotenv()\n",
    "MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "genai.configure(\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"), \n",
    "    transport='rest',\n",
    "    client_options=ClientOptions(api_endpoint=\"https://api.metisai.ir\")\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    MODEL_NAME,\n",
    "    system_instruction=\"\"\"\n",
    "    Your task is to answer factual questions accurately and concisely, \n",
    "    using only the information you have learned. You will be shown examples of questions\n",
    "      and their correct answers. Use them as a guide to understand how to answer similar questions.\n",
    "\n",
    "    Do not explain your answer, list evidence, or include extra text. Only return the final answer.\n",
    "    below are the answer types that you can use:\n",
    "    اسامی عام\n",
    "    شخص\n",
    "    بلی/خیر\n",
    "    تاریخ\n",
    "    رویداد\n",
    "    مکان\n",
    "    اسامی خاص دیگر\n",
    "    شماره\n",
    "    کار هنری\n",
    "    گروه یا سازمان\n",
    "    صفت\n",
    "\n",
    "\n",
    "    Examples:\n",
    "    Q: در مقابل تبی که بنتونیت با نام گل ارمنی در آن معروف است چه چیز قرار دارد ؟\n",
    "    <ANSWER> پزشکی مبتنی بر شواهد</ANSWER>\n",
    "\n",
    "    Q: کدام یک از ستارگان فیلم زنی پشت پنجره بازیکن هاکی نیز می باشد ؟\n",
    "    <ANSWER>وایـت راسل</ANSWER>\n",
    "\n",
    "    Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟\n",
    "    <ANSWER>بلی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد در چه سالی به فضا پرتاب شد ؟\n",
    "    <ANSWER>سال ۱۹۸۲</ANSWER>\n",
    "\n",
    "    Q: جایزه ای که سیدیبه در سال 2007 آن را دریافت کرد متعلق به کدام جشنواره است ؟\n",
    "    <ANSWER>جشنوارهٔ فیلم ونیز</ANSWER>\n",
    "\n",
    "    Q: از بین کشورهای پرتغال و اسرائیل در کدام یک زودتر مسابقه آواز یوروویژن برگزار شد ؟\n",
    "    <ANSWER>پرتغال</ANSWER>\n",
    "\n",
    "    Q: کدام یک از زبان هایی که کلمه حمد در آن استفاده می شود دارای وضعیت رسمی در چندین ایالت هند نیز هست ؟\n",
    "    <ANSWER>زبان اردو</ANSWER>\n",
    "\n",
    "    Q: عدد اتمی عنصری که باعث میشه گاز نئون لامپ‌ها در صورت استفاده از آن رنگ آبی روشن تولید کنند چیست ؟\n",
    "    <ANSWER>۸۰</ANSWER>\n",
    "\n",
    "    Q: بازیگری امریکایی که در 23 ژوئیه 1967 بدنیا امد با بازی در چه فیلمی اولین بفتای خود را گرفت ؟\n",
    "    <ANSWER>فیلم کاپوتی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد توسط چه سازمانی مدیریت می شود ؟\n",
    "    <ANSWER>ناسا</ANSWER>\n",
    "\n",
    "    Q: تماشاگران کمپانی که استفنی مک‌من لوک از سال 2022 رئیس آن بود را چه کسانی تشکیل می دهند ؟\n",
    "    <ANSWER>افراد جوان تا پیر</ANSWER>\n",
    "\n",
    "    Q: نام دیگر قومی که پیروز یکم آن ها را به طور قطعی شکست داد چیست ؟\n",
    "    <ANSWER>قوم کیدار</ANSWER>\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [11:33<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for item in tqdm(test_data):\n",
    "   question = item['question']\n",
    "   answer = item['answer']\n",
    "   _id = item['id']\n",
    "   response = model.generate_content(\n",
    "       question\n",
    "   )\n",
    "   model_answer = response.candidates[0].content.parts[0].text\n",
    "   results.append({\n",
    "       'question': question,\n",
    "       'answer': answer,\n",
    "       'model_answer': model_answer,\n",
    "       'id': _id,\n",
    "   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers:   0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers: 100%|██████████| 152/152 [02:19<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    is_correct: bool = Field(description=\"True if the model answer is semantically equivalent to the correct answer, False otherwise\")\n",
    "\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Using gpt-4o-mini for better performance and cost efficiency\n",
    "    temperature=0.1,  \n",
    "    max_tokens=500,\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"),\n",
    "    base_url='https://api.metisai.ir/openai/v1',\n",
    ").with_structured_output(AnswerEvaluation)\n",
    "\n",
    "def clean_model_answer(model_answer: str) -> str:\n",
    "    # Remove <ANSWER> tags\n",
    "    cleaned = re.sub(r'<ANSWER>(.*?)</ANSWER>', r'\\1', model_answer, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def evaluate_single_answer(question: str, correct_answer: str, model_answer: str) -> AnswerEvaluation:\n",
    "    \"\"\"Evaluate a single question-answer pair using LLM judge\"\"\"\n",
    "    \n",
    "    clean_model_answer_text = clean_model_answer(model_answer)\n",
    "    \n",
    "    # Create the evaluation prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert judge evaluating Persian/Farsi question-answer pairs. \n",
    "Your task is to determine if the model's answer is semantically equivalent to the correct answer.\n",
    "\n",
    "Consider these factors:\n",
    "- Semantic similarity (same meaning, different wording)\n",
    "- Spelling variations and Persian writing differences\n",
    "- Different but equivalent expressions (e.g., \"بله\" vs \"بلی\", both mean \"yes\")\n",
    "- Context and cultural nuances in Persian language\n",
    "- Both answers should convey the same factual information\n",
    "\n",
    "Be strict but fair - minor spelling differences or equivalent expressions should be considered correct.\n",
    "Only mark as incorrect if the meaning is genuinely different or wrong.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Evaluate if the model answer is semantically equivalent to the correct answer.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt with the actual values\n",
    "    messages = prompt.format_messages(\n",
    "        question=question,\n",
    "        correct_answer=correct_answer,\n",
    "        model_answer=clean_model_answer_text\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        evaluation = llm_judge.invoke(messages)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        return AnswerEvaluation(\n",
    "            is_correct=False,\n",
    "            reasoning=f\"API call failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "evaluated_results = []\n",
    "for item in tqdm(results, desc=\"Evaluating answers\"):\n",
    "    \n",
    "    question = item['question']\n",
    "    correct_answer = item['answer']\n",
    "    model_answer = item['model_answer']\n",
    "    item_id = item['id']\n",
    "    \n",
    "    evaluation = evaluate_single_answer(question, correct_answer, model_answer)\n",
    "    \n",
    "    evaluated_item = {\n",
    "        **item,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': evaluation.is_correct,\n",
    "    }\n",
    "    \n",
    "    evaluated_results.append(evaluated_item)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(evaluated_results)\n",
    "df.to_csv('evaluated_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5460526315789473"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(df[df['is_correct'] == True])/ len(df['is_correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "genai.configure(\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"), \n",
    "    transport='rest',\n",
    "    client_options=ClientOptions(api_endpoint=\"https://api.metisai.ir\")\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    MODEL_NAME,\n",
    "    system_instruction=\"\"\"\n",
    "    Your task is to answer factual questions accurately. For each question, first think step-by-step using only the knowledge you have learned. Write your reasoning inside a <Think>...</Think> block. Then, provide the final answer in a <ANSWER>...</ANSWER> block.\n",
    "\n",
    "    Use the examples below as a guide. Only use information that would be reasonably known or inferred from training. Do not hallucinate or reference sources.\n",
    "\n",
    "    Below are the answer types you may use (depending on the question):\n",
    "    اسامی عام  \n",
    "    شخص  \n",
    "    بلی/خیر  \n",
    "    تاریخ  \n",
    "    رویداد  \n",
    "    مکان  \n",
    "    اسامی خاص دیگر  \n",
    "    شماره  \n",
    "    کار هنری  \n",
    "    گروه یا سازمان  \n",
    "    صفت  \n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Q: در مقابل تبی که بنتونیت با نام گل ارمنی در آن معروف است چه چیز قرار دارد ؟  \n",
    "    <Think>بنتونیت در طب سنتی شناخته می‌شود. در تقابل با طب سنتی معمولاً پزشکی مدرن یا پزشکی مبتنی بر شواهد قرار دارد.</Think>  \n",
    "    <ANSWER>پزشکی مبتنی بر شواهد</ANSWER>\n",
    "\n",
    "    Q: کدام یک از ستارگان فیلم زنی پشت پنجره بازیکن هاکی نیز می باشد ؟  \n",
    "    <Think>در لیست بازیگران فیلم زنی پشت پنجره، وایت راسل حضور دارد. او همچنین بازیکن سابق هاکی است.</Think>  \n",
    "    <ANSWER>وایـت راسل</ANSWER>\n",
    "\n",
    "    Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟  \n",
    "    <Think>سایتو دوسان یک سامورایی بوده و سامورایی‌ها نجیب‌زادگان نظامی در ژاپن هستند.</Think>  \n",
    "    <ANSWER>بلی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد در چه سالی به فضا پرتاب شد ؟  \n",
    "    <Think>نقشه‌بردار موضوعی (TM) برای اولین بار بر روی ماهواره لندست ۴ نصب شد. این ماهواره در سال ۱۹۸۲ پرتاب شد.</Think>  \n",
    "    <ANSWER>سال ۱۹۸۲</ANSWER>\n",
    "\n",
    "    Q: جایزه ای که سیدیبه در سال 2007 آن را دریافت کرد متعلق به کدام جشنواره است ؟  \n",
    "    <Think>سیدیبه در سال ۲۰۰۷ جایزه شیر طلایی دریافت کرد که متعلق به جشنواره فیلم ونیز است.</Think>  \n",
    "    <ANSWER>جشنوارهٔ فیلم ونیز</ANSWER>\n",
    "\n",
    "    Q: تماشاگران کمپانی که استفنی مک‌من لوک از سال 2022 رئیس آن بود را چه کسانی تشکیل می‌دهند ؟  \n",
    "    <Think>استفنی مک‌من رئیس دبلیودبلیوئی بوده که برنامه‌هایش در کشورهای مختلف پخش می‌شود و طبق آمار مخاطبانش از رده سنی ۲ تا ۵۰ سال به بالا هستند.</Think>  \n",
    "    <ANSWER>افراد جوان تا پیر</ANSWER>\n",
    "\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [06:50<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for item in tqdm(test_data):\n",
    "   question = item['question']\n",
    "   answer = item['answer']\n",
    "   _id = item['id']\n",
    "   response = model.generate_content(\n",
    "       question\n",
    "   )\n",
    "   model_answer = response.candidates[0].content.parts[0].text\n",
    "   results.append({\n",
    "       'question': question,\n",
    "       'answer': answer,\n",
    "       'model_answer': model_answer,\n",
    "       'id': _id,\n",
    "   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('results_reasoning.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers:   0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers: 100%|██████████| 152/152 [02:26<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    is_correct: bool = Field(description=\"True if the model answer is semantically equivalent to the correct answer, False otherwise\")\n",
    "\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Using gpt-4o-mini for better performance and cost efficiency\n",
    "    temperature=0.1,  # Low temperature for consistent judgments\n",
    "    max_tokens=500,\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"),\n",
    "    base_url='https://api.metisai.ir/openai/v1',\n",
    ").with_structured_output(AnswerEvaluation)\n",
    "\n",
    "def clean_model_answer(model_answer: str) -> str:\n",
    "    # Remove <ANSWER> tags\n",
    "    \"\"\"Clean the model answer by extracting text from <ANSWER> tags\"\"\"\n",
    "    if not model_answer:\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find <ANSWER> tags first\n",
    "    answer_match = re.search(r'<ANSWER>(.*?)</ANSWER>', model_answer, re.DOTALL | re.IGNORECASE)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    \n",
    "    return model_answer.strip()\n",
    "\n",
    "def evaluate_single_answer(question: str, correct_answer: str, model_answer: str) -> AnswerEvaluation:\n",
    "    \"\"\"Evaluate a single question-answer pair using LLM judge\"\"\"\n",
    "    \n",
    "    clean_model_answer_text = clean_model_answer(model_answer)\n",
    "    \n",
    "    # Create the evaluation prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert judge evaluating Persian/Farsi question-answer pairs. \n",
    "Your task is to determine if the model's answer is semantically equivalent to the correct answer.\n",
    "\n",
    "Consider these factors:\n",
    "- Semantic similarity (same meaning, different wording)\n",
    "- Spelling variations and Persian writing differences\n",
    "- Different but equivalent expressions (e.g., \"بله\" vs \"بلی\", both mean \"yes\")\n",
    "- Context and cultural nuances in Persian language\n",
    "- Both answers should convey the same factual information\n",
    "\n",
    "Be strict but fair - minor spelling differences or equivalent expressions should be considered correct.\n",
    "Only mark as incorrect if the meaning is genuinely different or wrong.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Evaluate if the model answer is semantically equivalent to the correct answer.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    messages = prompt.format_messages(\n",
    "        question=question,\n",
    "        correct_answer=correct_answer,\n",
    "        model_answer=clean_model_answer_text\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        evaluation = llm_judge.invoke(messages)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        return AnswerEvaluation(\n",
    "            is_correct=False,\n",
    "            reasoning=f\"API call failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "evaluated_results = []\n",
    "for item in tqdm(results, desc=\"Evaluating answers\"):\n",
    "    \n",
    "    question = item['question']\n",
    "    correct_answer = item['answer']\n",
    "    model_answer = item['model_answer']\n",
    "    item_id = item['id']\n",
    "    \n",
    "    evaluation = evaluate_single_answer(question, correct_answer, model_answer)\n",
    "    \n",
    "    evaluated_item = {\n",
    "        **item,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': evaluation.is_correct,\n",
    "    }\n",
    "    \n",
    "    evaluated_results.append(evaluated_item)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(evaluated_results)\n",
    "df.to_csv('evaluated_results_reasoning.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQUAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from tqdm import tqdm\n",
    "genai.configure(\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"), \n",
    "    transport='rest',\n",
    "    client_options=ClientOptions(api_endpoint=\"https://api.metisai.ir\")\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    MODEL_NAME,\n",
    "    system_instruction=\"\"\"\n",
    "    Your task is to answer factual questions accurately and concisely, \n",
    "    using only the information you have learned. You will be shown examples of questions\n",
    "      and their correct answers. Use them as a guide to understand how to answer similar questions.\n",
    "\n",
    "    Do not explain your answer, list evidence, or include extra text. Only return the final answer.\n",
    "\n",
    "    Examples:\n",
    "    Q: در مقابل تبی که بنتونیت با نام گل ارمنی در آن معروف است چه چیز قرار دارد ؟\n",
    "    <ANSWER> پزشکی مبتنی بر شواهد</ANSWER>\n",
    "\n",
    "    Q: کدام یک از ستارگان فیلم زنی پشت پنجره بازیکن هاکی نیز می باشد ؟\n",
    "    <ANSWER>وایـت راسل</ANSWER>\n",
    "\n",
    "    Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟\n",
    "    <ANSWER>بلی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد در چه سالی به فضا پرتاب شد ؟\n",
    "    <ANSWER>سال ۱۹۸۲</ANSWER>\n",
    "\n",
    "    Q: جایزه ای که سیدیبه در سال 2007 آن را دریافت کرد متعلق به کدام جشنواره است ؟\n",
    "    <ANSWER>جشنوارهٔ فیلم ونیز</ANSWER>\n",
    "\n",
    "    Q: از بین کشورهای پرتغال و اسرائیل در کدام یک زودتر مسابقه آواز یوروویژن برگزار شد ؟\n",
    "    <ANSWER>پرتغال</ANSWER>\n",
    "\n",
    "    Q: کدام یک از زبان هایی که کلمه حمد در آن استفاده می شود دارای وضعیت رسمی در چندین ایالت هند نیز هست ؟\n",
    "    <ANSWER>زبان اردو</ANSWER>\n",
    "\n",
    "    Q: عدد اتمی عنصری که باعث میشه گاز نئون لامپ‌ها در صورت استفاده از آن رنگ آبی روشن تولید کنند چیست ؟\n",
    "    <ANSWER>۸۰</ANSWER>\n",
    "\n",
    "    Q: بازیگری امریکایی که در 23 ژوئیه 1967 بدنیا امد با بازی در چه فیلمی اولین بفتای خود را گرفت ؟\n",
    "    <ANSWER>فیلم کاپوتی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد توسط چه سازمانی مدیریت می شود ؟\n",
    "    <ANSWER>ناسا</ANSWER>\n",
    "\n",
    "    Q: تماشاگران کمپانی که استفنی مک‌من لوک از سال 2022 رئیس آن بود را چه کسانی تشکیل می دهند ؟\n",
    "    <ANSWER>افراد جوان تا پیر</ANSWER>\n",
    "\n",
    "    Q: نام دیگر قومی که پیروز یکم آن ها را به طور قطعی شکست داد چیست ؟\n",
    "    <ANSWER>قوم کیدار</ANSWER>\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('../../data/pquad/pquad_questions.csv', encoding='utf-8').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [19:11<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in tqdm(test_data):\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    _id = item['id']\n",
    "    context_id = item['context_id']\n",
    "    response = model.generate_content(\n",
    "        question\n",
    "    )\n",
    "    try:\n",
    "        model_answer = response.candidates[0].content.parts[0].text\n",
    "    except Exception as e:\n",
    "        model_answer = \"Error: \" + str(e)\n",
    "      \n",
    "    results.append({\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'model_answer': model_answer,\n",
    "        'id': _id,\n",
    "        'context_id': context_id,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('pquad_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers: 100%|██████████| 500/500 [07:49<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    is_correct: bool = Field(description=\"True if the model answer is semantically equivalent to the correct answer, False otherwise\")\n",
    "\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  \n",
    "    temperature=0.1,  \n",
    "    max_tokens=500,\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"),\n",
    "    base_url='https://api.metisai.ir/openai/v1',\n",
    ").with_structured_output(AnswerEvaluation)\n",
    "\n",
    "def clean_model_answer(model_answer: str) -> str:\n",
    "    # Remove <ANSWER> tags\n",
    "    cleaned = re.sub(r'<ANSWER>(.*?)</ANSWER>', r'\\1', model_answer, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def evaluate_single_answer(question: str, correct_answer: str, model_answer: str) -> AnswerEvaluation:\n",
    "    \"\"\"Evaluate a single question-answer pair using LLM judge\"\"\"\n",
    "    \n",
    "    clean_model_answer_text = clean_model_answer(model_answer)\n",
    "    \n",
    "    # Create the evaluation prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert judge evaluating Persian/Farsi question-answer pairs. \n",
    "Your task is to determine if the model's answer is semantically equivalent to the correct answer.\n",
    "\n",
    "Consider these factors:\n",
    "- Semantic similarity (same meaning, different wording)\n",
    "- Spelling variations and Persian writing differences\n",
    "- Different but equivalent expressions (e.g., \"بله\" vs \"بلی\", both mean \"yes\")\n",
    "- Context and cultural nuances in Persian language\n",
    "- Both answers should convey the same factual information\n",
    "\n",
    "Be strict but fair - minor spelling differences or equivalent expressions should be considered correct.\n",
    "Only mark as incorrect if the meaning is genuinely different or wrong.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Evaluate if the model answer is semantically equivalent to the correct answer.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt with the actual values\n",
    "    messages = prompt.format_messages(\n",
    "        question=question,\n",
    "        correct_answer=correct_answer,\n",
    "        model_answer=clean_model_answer_text\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        evaluation = llm_judge.invoke(messages)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        return AnswerEvaluation(\n",
    "            is_correct=False,\n",
    "            reasoning=f\"API call failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "evaluated_results = []\n",
    "for item in tqdm(results, desc=\"Evaluating answers\"):\n",
    "    \n",
    "    question = item['question']\n",
    "    correct_answer = item['answer']\n",
    "    model_answer = item['model_answer']\n",
    "    item_id = item['id']\n",
    "    \n",
    "    evaluation = evaluate_single_answer(question, correct_answer, model_answer)\n",
    "    \n",
    "    evaluated_item = {\n",
    "        **item,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': evaluation.is_correct,\n",
    "    }\n",
    "    \n",
    "    evaluated_results.append(evaluated_item)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(evaluated_results)\n",
    "df.to_csv('pqaud_evaluated_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['is_correct'] == True])/ len(df['is_correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "genai.configure(\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"), \n",
    "    transport='rest',\n",
    "    client_options=ClientOptions(api_endpoint=\"https://api.metisai.ir\")\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    MODEL_NAME,\n",
    "    system_instruction=\"\"\"\n",
    "    Your task is to answer factual questions accurately. For each question, first think step-by-step using only the knowledge you have learned. Write your reasoning inside a <Think>...</Think> block. Then, provide the final answer in a <ANSWER>...</ANSWER> block.\n",
    "\n",
    "    Use the examples below as a guide. Only use information that would be reasonably known or inferred from training. Do not hallucinate or reference sources. \n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Q: در مقابل تبی که بنتونیت با نام گل ارمنی در آن معروف است چه چیز قرار دارد ؟  \n",
    "    <Think>بنتونیت در طب سنتی شناخته می‌شود. در تقابل با طب سنتی معمولاً پزشکی مدرن یا پزشکی مبتنی بر شواهد قرار دارد.</Think>  \n",
    "    <ANSWER>پزشکی مبتنی بر شواهد</ANSWER>\n",
    "\n",
    "    Q: کدام یک از ستارگان فیلم زنی پشت پنجره بازیکن هاکی نیز می باشد ؟  \n",
    "    <Think>در لیست بازیگران فیلم زنی پشت پنجره، وایت راسل حضور دارد. او همچنین بازیکن سابق هاکی است.</Think>  \n",
    "    <ANSWER>وایـت راسل</ANSWER>\n",
    "\n",
    "    Q: آیا سایتو دوسان یک نجیب زاده نظامی است ؟  \n",
    "    <Think>سایتو دوسان یک سامورایی بوده و سامورایی‌ها نجیب‌زادگان نظامی در ژاپن هستند.</Think>  \n",
    "    <ANSWER>بلی</ANSWER>\n",
    "\n",
    "    Q: داده های ماهواره ای که نخستین‌ بار نقشه‌بردار موضوعی بر روی آن نصب شد در چه سالی به فضا پرتاب شد ؟  \n",
    "    <Think>نقشه‌بردار موضوعی (TM) برای اولین بار بر روی ماهواره لندست ۴ نصب شد. این ماهواره در سال ۱۹۸۲ پرتاب شد.</Think>  \n",
    "    <ANSWER>سال ۱۹۸۲</ANSWER>\n",
    "\n",
    "    Q: جایزه ای که سیدیبه در سال 2007 آن را دریافت کرد متعلق به کدام جشنواره است ؟  \n",
    "    <Think>سیدیبه در سال ۲۰۰۷ جایزه شیر طلایی دریافت کرد که متعلق به جشنواره فیلم ونیز است.</Think>  \n",
    "    <ANSWER>جشنوارهٔ فیلم ونیز</ANSWER>\n",
    "\n",
    "    Q: تماشاگران کمپانی که استفنی مک‌من لوک از سال 2022 رئیس آن بود را چه کسانی تشکیل می‌دهند ؟  \n",
    "    <Think>استفنی مک‌من رئیس دبلیودبلیوئی بوده که برنامه‌هایش در کشورهای مختلف پخش می‌شود و طبق آمار مخاطبانش از رده سنی ۲ تا ۵۰ سال به بالا هستند.</Think>  \n",
    "    <ANSWER>افراد جوان تا پیر</ANSWER>\n",
    "\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [14:49<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for item in tqdm(test_data):\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    _id = item['id']\n",
    "    response = model.generate_content(\n",
    "        question\n",
    "    )\n",
    "    try:\n",
    "       model_answer = response.candidates[0].content.parts[0].text\n",
    "    except Exception as e:\n",
    "         model_answer = \"Error: \" + str(e)\n",
    "    results.append({\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'model_answer': model_answer,\n",
    "        'id': _id,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('pquad_results_reasoning.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating answers: 100%|██████████| 500/500 [08:56<00:00,  1.07s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AnswerEvaluation(BaseModel):\n",
    "    is_correct: bool = Field(description=\"True if the model answer is semantically equivalent to the correct answer, False otherwise\")\n",
    "\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.1,  \n",
    "    max_tokens=500,\n",
    "    api_key=os.getenv(\"METIS_API_KEY\"),\n",
    "    base_url='https://api.metisai.ir/openai/v1',\n",
    ").with_structured_output(AnswerEvaluation)\n",
    "\n",
    "def clean_model_answer(model_answer: str) -> str:\n",
    "    # Remove <ANSWER> tags\n",
    "    \"\"\"Clean the model answer by extracting text from <ANSWER> tags\"\"\"\n",
    "    if not model_answer:\n",
    "        return \"\"\n",
    "    \n",
    "    answer_match = re.search(r'<ANSWER>(.*?)</ANSWER>', model_answer, re.DOTALL | re.IGNORECASE)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "    \n",
    "    return model_answer.strip()\n",
    "\n",
    "def evaluate_single_answer(question: str, correct_answer: str, model_answer: str) -> AnswerEvaluation:\n",
    "    \"\"\"Evaluate a single question-answer pair using LLM judge\"\"\"\n",
    "    \n",
    "    clean_model_answer_text = clean_model_answer(model_answer)\n",
    "    \n",
    "    # Create the evaluation prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert judge evaluating Persian/Farsi question-answer pairs. \n",
    "Your task is to determine if the model's answer is semantically equivalent to the correct answer.\n",
    "\n",
    "Consider these factors:\n",
    "- Semantic similarity (same meaning, different wording)\n",
    "- Spelling variations and Persian writing differences\n",
    "- Different but equivalent expressions (e.g., \"بله\" vs \"بلی\", both mean \"yes\")\n",
    "- Context and cultural nuances in Persian language\n",
    "- Both answers should convey the same factual information\n",
    "\n",
    "Be strict but fair - minor spelling differences or equivalent expressions should be considered correct.\n",
    "Only mark as incorrect if the meaning is genuinely different or wrong.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Evaluate if the model answer is semantically equivalent to the correct answer.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt with the actual values\n",
    "    messages = prompt.format_messages(\n",
    "        question=question,\n",
    "        correct_answer=correct_answer,\n",
    "        model_answer=clean_model_answer_text\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        evaluation = llm_judge.invoke(messages)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        return AnswerEvaluation(\n",
    "            is_correct=False,\n",
    "            reasoning=f\"API call failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "evaluated_results = []\n",
    "for item in tqdm(results, desc=\"Evaluating answers\"):\n",
    "    \n",
    "    question = item['question']\n",
    "    correct_answer = item['answer']\n",
    "    model_answer = item['model_answer']\n",
    "    item_id = item['id']\n",
    "    \n",
    "    evaluation = evaluate_single_answer(question, correct_answer, model_answer)\n",
    "    \n",
    "    evaluated_item = {\n",
    "        **item,\n",
    "        'clean_model_answer': clean_model_answer(model_answer),\n",
    "        'is_correct': evaluation.is_correct,\n",
    "    }\n",
    "    \n",
    "    evaluated_results.append(evaluated_item)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(evaluated_results)\n",
    "df.to_csv('pquad_evaluated_results_reasoning.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.296"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['is_correct'] == True])/ len(df['is_correct'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
