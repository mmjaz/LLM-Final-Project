{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62214b6",
   "metadata": {},
   "source": [
    "# Calculate EM, Precision, Recall, and F1 Score for PQuad HippoRAG Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc0f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fedab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/HippoRAG/HippoRAG_PQuad/evaluated_results_no_reasoning_RAG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fdeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation + \"،\")\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def convert_digits_en2fa(text):\n",
    "    english_digits = '0123456789'\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹'\n",
    "    \n",
    "    translation_table = str.maketrans(english_digits, persian_digits)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "\n",
    "def parse_result(gold, generated_text) -> bool:\n",
    "    if pd.isna(gold) or pd.isna(generated_text):\n",
    "        return False\n",
    "        \n",
    "    gold = str(gold)\n",
    "    generated_text = str(generated_text)\n",
    "    \n",
    "    gold = convert_digits_en2fa(gold)\n",
    "    generated_text = convert_digits_en2fa(generated_text)\n",
    "\n",
    "    if gold in generated_text:\n",
    "        return True\n",
    "    if gold == \"بلی\":\n",
    "        if \"بله\" in generated_text:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8437c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    # Handle None/NaN values\n",
    "    if pd.isna(prediction) or pd.isna(ground_truth):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    prediction = str(prediction)\n",
    "    ground_truth = str(ground_truth)\n",
    "    \n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    # Handle special cases for yes/no answers\n",
    "    if normalized_prediction in ['بله', 'خیر', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['بله', 'خیر', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    \n",
    "    # If either is empty, return zeros\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return ZERO_METRIC\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    # Handle None/NaN values\n",
    "    if pd.isna(prediction) or pd.isna(ground_truth):\n",
    "        return False\n",
    "    \n",
    "    prediction = str(prediction)\n",
    "    ground_truth = str(ground_truth)\n",
    "    \n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c64c28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "parse_result_scores = []\n",
    "\n",
    "# Calculate metrics for each row\n",
    "for index, row in df.iterrows():\n",
    "    answer = row['answer']\n",
    "    model_answer = row['model_answer'] \n",
    "    \n",
    "    # Calculate EM\n",
    "    em = exact_match_score(model_answer, answer)\n",
    "    em_scores.append(em)\n",
    "    \n",
    "    # Calculate F1, Precision, Recall\n",
    "    f1, prec, rec = f1_score(model_answer, answer)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(prec)\n",
    "    recall_scores.append(rec)\n",
    "    \n",
    "    # Calculate parse result (substring matching)\n",
    "    parse_res = parse_result(answer, model_answer)\n",
    "    parse_result_scores.append(parse_res)\n",
    "\n",
    "# Add the calculated metrics to the dataframe\n",
    "df['em_score'] = em_scores\n",
    "df['f1_score'] = f1_scores\n",
    "df['precision_score'] = precision_scores\n",
    "df['recall_score'] = recall_scores\n",
    "df['parse_result'] = parse_result_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25c6c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 500\n",
      "Questions with valid answers: 404\n",
      "Questions with empty/null answers: 96\n",
      "\n",
      "=== Overall Metrics (on 404 valid questions) ===\n",
      "Exact Match (EM): 0.1460\n",
      "F1 Score: 0.3083\n",
      "Precision: 0.3260\n",
      "Recall: 0.3312\n",
      "Parse Accuracy: 0.2252\n",
      "LLM Judge Accuracy (is_correct): 0.3564\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall aggregated metrics\n",
    "total_questions = len(df)\n",
    "\n",
    "# Filter out questions with empty/null answers for meaningful metric calculation\n",
    "valid_questions = df.dropna(subset=['answer'])\n",
    "valid_questions = valid_questions[valid_questions['answer'] != '']\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Questions with valid answers: {len(valid_questions)}\")\n",
    "print(f\"Questions with empty/null answers: {total_questions - len(valid_questions)}\")\n",
    "\n",
    "\n",
    "# Calculate metrics on valid questions only\n",
    "overall_em = valid_questions['em_score'].mean()\n",
    "overall_f1 = valid_questions['f1_score'].mean()\n",
    "overall_precision = valid_questions['precision_score'].mean()\n",
    "overall_recall = valid_questions['recall_score'].mean()\n",
    "overall_parse_accuracy = valid_questions['parse_result'].mean()\n",
    "\n",
    "overall_is_correct = valid_questions['is_correct'].mean()\n",
    "\n",
    "print(f\"\\n=== Overall Metrics (on {len(valid_questions)} valid questions) ===\")\n",
    "print(f\"Exact Match (EM): {overall_em:.4f}\")\n",
    "print(f\"F1 Score: {overall_f1:.4f}\")\n",
    "print(f\"Precision: {overall_precision:.4f}\")\n",
    "print(f\"Recall: {overall_recall:.4f}\")\n",
    "print(f\"Parse Accuracy: {overall_parse_accuracy:.4f}\")\n",
    "print(f\"LLM Judge Accuracy (is_correct): {overall_is_correct:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
